{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts4GtbHMN4Dy"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit --q\n",
        "\n",
        "import rdkit\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "# Generate the Morgan Fingerprint generator\n",
        "morgan_generator = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
        "\n",
        "# Utils\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Data Handling\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset,random_split\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "\n",
        "# Transformer  # // https://arxiv.org/pdf/2010.09885 // #\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Analysis\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gdown\n",
        "# Set pandas to display all columns\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "# Utilities for ML\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    classification_report,f1_score,matthews_corrcoef,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        "    auc\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YZ44GxB_uTR",
        "outputId": "b981417f-eabb-4da5-d05f-a4d17119192e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX2FDFwJkbx_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbw78dh0EXYo"
      },
      "outputs": [],
      "source": [
        "def generate_mol(smile):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smile)\n",
        "        if mol is None:\n",
        "            print('Error generating molecule from SMILES')\n",
        "        return mol\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating molecule from SMILES\")\n",
        "        return None\n",
        "\n",
        "def generate_morgan_fingerprint(mol):\n",
        "    # Generate the fingerprint as a bit vector\n",
        "    fp = morgan_generator.GetFingerprint(mol)\n",
        "\n",
        "    # Convert the bit vector to a numpy array of 0s and 1s\n",
        "    arr = np.zeros((2048,), dtype=int)\n",
        "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "\n",
        "    return arr\n",
        "\n",
        "def get_maccs(mol):\n",
        "    descriptors = {}\n",
        "    # Add MACCS Fingerprint\n",
        "    maccs_fp = AllChem.GetMACCSKeysFingerprint(mol)\n",
        "    maccs_arr = np.zeros((167,), dtype=int)\n",
        "    DataStructs.ConvertToNumpyArray(maccs_fp, maccs_arr)\n",
        "    descriptors['MACCSFP'] = maccs_arr\n",
        "    # Include MACCS fingerprint bits as individual descriptors\n",
        "    for i, bit in enumerate(maccs_arr):\n",
        "        descriptors[f'MACCSFP_bit_{i}'] = bit\n",
        "    return descriptors\n",
        "\n",
        "# ChemBerta Tokenizer and Predictive Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('seyonec/ChemBERTa_zinc250k_v2_40k')\n",
        "model = AutoModel.from_pretrained('seyonec/ChemBERTa_zinc250k_v2_40k')\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Generating the embeddings from ChemBerta\n",
        "def embed_smiles(smiles):\n",
        "    inputs = tokenizer(smiles, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state\n",
        "    # You can take the mean of the embeddings or another summarization technique\n",
        "    return embeddings.cpu().numpy()\n",
        "\n",
        "# Mean Pooling the Embeddings\n",
        "def mean_pooling(embeddings):\n",
        "    # Removing the batch dimension\n",
        "    embeddings = np.squeeze(embeddings, axis=0)  # Shape: (seq, 768)\n",
        "    # Computing the mean over the sequence dimension (tokens)\n",
        "    mean_embedding = embeddings.mean(axis=0)  # Shape: (768,)\n",
        "    return mean_embedding\n",
        "\n",
        "# Generate all Features for Classification\n",
        "def extract_features_for_class(smiles):\n",
        "    mol = generate_mol(smile)\n",
        "    maccs_dict = get_maccs(mol)\n",
        "    maccs = torch.tensor(maccs_dict['MACCSFP'].tolist(), dtype=torch.float32).to(device)  # Convert dict values to tensor\n",
        "    morgan = torch.tensor(generate_morgan_fingerprint(mol), dtype=torch.float32).to(device)\n",
        "    embeds = torch.tensor(mean_pooling(embed_smiles(smiles)), dtype=torch.float32).to(device)\n",
        "    return mol, maccs, morgan, embeds\n",
        "\n",
        "def classify_permeability(smiles):\n",
        "\n",
        "  mol, maccs, morgan, embeds = extract_features_for_class(smiles)\n",
        "\n",
        "  if mol is None:\n",
        "    return np.Nan , np.Nan\n",
        "\n",
        "  features = torch.cat([maccs, morgan, embeds],dim = 0 )\n",
        "\n",
        "  # Convert to PyTorch tensor\n",
        "  features_tensor = torch.tensor(features, dtype=torch.float32)  # Shape: (total_dim,)\n",
        "\n",
        "  # Add batch dimension\n",
        "  features_tensor = features_tensor.unsqueeze(0)\n",
        "  features_tensor.size()\n",
        "\n",
        "  cnn_model.eval()\n",
        "  with torch.no_grad():\n",
        "      output = torch.sigmoid(cnn_model(features_tensor))\n",
        "\n",
        "  return output, mol"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Model"
      ],
      "metadata": {
        "id": "pTGPEyjxLpc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_dim, out_channels, classification_layer_dim = 64 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.out_channels = out_channels\n",
        "        self.class_dim =  classification_layer_dim\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=self.out_channels * 4, kernel_size=3, stride=1, padding=2),\n",
        "            nn.BatchNorm1d(self.out_channels * 4),\n",
        "            nn.PReLU(self.out_channels*4),\n",
        "            nn.AvgPool1d(kernel_size=3, stride=2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Conv1d(in_channels= self.out_channels * 4 , out_channels=self.out_channels *2 , kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm1d(self.out_channels * 2),\n",
        "            nn.PReLU(self.out_channels*2),\n",
        "            nn.AvgPool1d(kernel_size=5, stride=2),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "\n",
        "        # Calculate the output size after convolutional layers\n",
        "        self._conv_output_size = self._get_conv_output()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self._conv_output_size, self.class_dim  ),\n",
        "            nn.BatchNorm1d(self.class_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(self.class_dim, self.class_dim //2 ),\n",
        "            nn.BatchNorm1d(self.class_dim//2),\n",
        "            nn.PReLU(self.class_dim//2),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(self.class_dim//2 , 1)\n",
        "        )\n",
        "\n",
        "    def _get_conv_output(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Computes the size of the output of the convolutional layers\n",
        "         to define the input size of the first linear layer.\n",
        "        \"\"\"\n",
        "        # Create a dummy input tensor with batch size 1\n",
        "        dummy_input = torch.zeros(1, 1, self.input_dim)\n",
        "        output_feat = self.conv(dummy_input)\n",
        "        output_size = output_feat.numel()\n",
        "        print(output_size)\n",
        "        return output_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, input_dim)\n",
        "        print('Starting Forward Method')\n",
        "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, input_dim)\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, conv_output_size)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "G0TL85w5OJEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly load the entire model\n",
        "cnn_model = torch.load('/content/drive/My Drive/cnn_bbb_entire_model.pth', map_location=torch.device('cpu'))\n",
        "\n",
        "# Move to device and set to evaluation mode\n",
        "model.to(device)\n",
        "model.eval()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25P6MgI9Lnyi",
        "outputId": "04c4516c-6e3b-4eb2-a458-63aa303650f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-fee8f30ebbbf>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cnn_model = torch.load('/content/drive/My Drive/cnn_bbb_entire_model.pth', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smile = 'CC(C)C1=C(C(=CC=C1)C(C)C)O.CNC1(CCCCC1=O)C2=CC=CC=C2Cl'"
      ],
      "metadata": {
        "id": "4lySuIxUOPBG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}